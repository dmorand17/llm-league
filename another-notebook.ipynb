{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning Llama 3.2 3B Instruct Model with SageMaker JumpStart\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Llama 3.2 3B instruct model on Amazon SageMaker using JumpStart. We'll use a small JSONL dataset (~100 rows) and leverage SageMaker JumpStart's pre-built containers and scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "import boto3\n",
        "import json\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
        "from sagemaker import get_execution_role"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup SageMaker Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Set up SageMaker session and role\n",
        "session = sagemaker.Session()\n",
        "role = get_execution_role()\n",
        "\n",
        "# Define S3 bucket for storing training data and model artifacts\n",
        "bucket = session.default_bucket()\n",
        "prefix = 'llama-3-2-finetuning'\n",
        "\n",
        "print(f\"SageMaker session is using bucket: {bucket}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare and Format Training Data\n",
        "\n",
        "We need to format our data according to the requirements of the Llama 3.2 model. Let's create a simple function to convert our dataset to the correct format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Sample data preparation\n",
        "# Assuming you have a jsonl file with 'prompt' and 'response' fields\n",
        "# You can replace this with loading your actual data\n",
        "\n",
        "def format_data_for_llama(input_file, output_file):\n",
        "    \"\"\"Convert data to Llama 3.2 chat format\"\"\"\n",
        "    formatted_data = []\n",
        "    \n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f:\n",
        "            example = json.loads(line)\n",
        "            \n",
        "            # Extract prompt and response\n",
        "            prompt = example.get('prompt', '')\n",
        "            response = example.get('response', '')\n",
        "            \n",
        "            # Create conversation in Llama 3.2 format\n",
        "            conversation = [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": response}\n",
        "            ]\n",
        "            \n",
        "            formatted_data.append({\"conversations\": conversation})\n",
        "    \n",
        "    # Write formatted data to output file\n",
        "    with open(output_file, 'w') as f:\n",
        "        for item in formatted_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    print(f\"Formatted {len(formatted_data)} examples to {output_file}\")\n",
        "    return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# For demonstration, let's create a simple sample dataset\n",
        "# In practice, you would use your own dataset file\n",
        "\n",
        "sample_data = [\n",
        "    {\"prompt\": \"What are the key features of your product?\", \"response\": \"Our product offers seamless integration, robust security, and an intuitive interface.\"},\n",
        "    {\"prompt\": \"How do I reset my password?\", \"response\": \"To reset your password, click on the 'Forgot Password' link on the login page and follow the instructions sent to your email.\"}\n",
        "]\n",
        "\n",
        "# Create a sample input file\n",
        "input_file = \"sample_data.jsonl\"\n",
        "with open(input_file, 'w') as f:\n",
        "    for item in sample_data:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "# Format the data for Llama 3.2\n",
        "formatted_file = \"formatted_data.jsonl\"\n",
        "format_data_for_llama(input_file, formatted_file)\n",
        "\n",
        "# Display the formatted data\n",
        "with open(formatted_file, 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(json.loads(line))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Training Data to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Upload formatted data to S3\n",
        "train_data_s3_path = session.upload_data(\n",
        "    path=formatted_file,\n",
        "    bucket=bucket,\n",
        "    key_prefix=f\"{prefix}/data\"\n",
        ")\n",
        "\n",
        "print(f\"Training data uploaded to: {train_data_s3_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure and Launch Fine-tuning Job with JumpStart\n",
        "\n",
        "Now, let's use SageMaker JumpStart to fine-tune the Llama 3.2 3B model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Define model ID for Llama 3.2 3B Instruct in JumpStart\n",
        "model_id = \"meta-textgeneration-llama-3-2-3b-instruct\"\n",
        "model_version = \"*\"\n",
        "\n",
        "# Configure hyperparameters for fine-tuning\n",
        "hyperparameters = {\n",
        "    \"max_steps\": \"300\",               # Small dataset, so limit steps\n",
        "    \"epoch\": \"3\",                     # 3 epochs for small dataset\n",
        "    \"learning_rate\": \"2e-4\",          # Slightly higher learning rate\n",
        "    \"per_device_train_batch_size\": \"4\", # Batch size per GPU\n",
        "    \"gradient_accumulation_steps\": \"2\", # Effective batch size = 8\n",
        "    \"max_seq_length\": \"2048\",         # Max sequence length\n",
        "    \n",
        "    # LoRA parameters\n",
        "    \"lora_alpha\": \"32\",               # LoRA scaling factor\n",
        "    \"lora_dropout\": \"0.05\",           # Dropout probability\n",
        "    \"lora_r\": \"16\",                   # LoRA rank\n",
        "    \n",
        "    # QLoRA parameters for efficient training\n",
        "    \"use_bnb_4bit\": \"True\",            # Use 4-bit quantization\n",
        "    \"use_peft\": \"True\",               # Use parameter-efficient fine-tuning\n",
        "    \n",
        "    # Training settings\n",
        "    \"save_strategy\": \"epoch\",         # Save checkpoint every epoch\n",
        "    \"warmup_ratio\": \"0.03\",           # Percentage of steps for learning rate warmup\n",
        "    \"hub_model_id\": None,              # Don't push to HF Hub\n",
        "    \"hub_private_repo\": \"False\",      # Don't use private repo\n",
        "    \"hub_token\": None,                # No HF token needed\n",
        "    \"push_to_hub\": \"False\",           # Don't push to HF Hub\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create JumpStart estimator for fine-tuning\n",
        "estimator = JumpStartEstimator(\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.g5.2xlarge\",  # 1 NVIDIA A10G GPU\n",
        "    hyperparameters=hyperparameters,\n",
        "    environment={\"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"3600\"},\n",
        "    disable_output_compression=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Start the fine-tuning job\n",
        "estimator.fit(\n",
        "    train_data_s3_path,\n",
        "    logs=True,\n",
        "    wait=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Fine-tuned Model\n",
        "\n",
        "After training completes, you can deploy the fine-tuned model as a SageMaker endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Deploy the model as an endpoint\n",
        "predictor = estimator.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.g5.xlarge\",\n",
        "    container_startup_health_check_timeout=600  # Longer timeout for model loading\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a test function\n",
        "def test_model(predictor, prompt):\n",
        "    payload = {\n",
        "        \"inputs\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"top_p\": 0.9,\n",
        "            \"temperature\": 0.7,\n",
        "            \"return_full_text\": False\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    response = predictor.predict(payload)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test with sample prompts\n",
        "test_prompt = \"What are the key features of your product?\"\n",
        "response = test_model(predictor, test_prompt)\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python Script Version\n",
        "\n",
        "Below is a standalone Python script version that can be run outside of a notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%%writefile finetune_llama_jumpstart.py\n",
        "import os\n",
        "import boto3\n",
        "import json\n",
        "import argparse\n",
        "import sagemaker\n",
        "from sagemaker.jumpstart.model import JumpStartModel\n",
        "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
        "from sagemaker import get_execution_role\n",
        "\n",
        "def format_data_for_llama(input_file, output_file):\n",
        "    \"\"\"Convert data to Llama 3.2 chat format\"\"\"\n",
        "    formatted_data = []\n",
        "    \n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f:\n",
        "            example = json.loads(line)\n",
        "            \n",
        "            # Extract prompt and response\n",
        "            prompt = example.get('prompt', '')\n",
        "            response = example.get('response', '')\n",
        "            \n",
        "            # Create conversation in Llama 3.2 format\n",
        "            conversation = [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": response}\n",
        "            ]\n",
        "            \n",
        "            formatted_data.append({\"conversations\": conversation})\n",
        "    \n",
        "    # Write formatted data to output file\n",
        "    with open(output_file, 'w') as f:\n",
        "        for item in formatted_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    print(f\"Formatted {len(formatted_data)} examples to {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune Llama 3.2 3B using SageMaker JumpStart\")\n",
        "    parser.add_argument(\"--input-file\", type=str, required=True, help=\"Path to input JSONL file\")\n",
        "    parser.add_argument(\"--s3-bucket\", type=str, help=\"S3 bucket name\")\n",
        "    parser.add_argument(\"--s3-prefix\", type=str, default=\"llama-3-2-finetuning\", help=\"S3 prefix for data and model\")\n",
        "    parser.add_argument(\"--instance-type\", type=str, default=\"ml.g5.2xlarge\", help=\"Training instance type\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=2e-4, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size per device\")\n",
        "    parser.add_argument(\"--deploy\", action=\"store_true\", help=\"Deploy model after training\")\n",
        "    parser.add_argument(\"--deploy-instance-type\", type=str, default=\"ml.g5.xlarge\", help=\"Deployment instance type\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Initialize SageMaker session\n",
        "    session = sagemaker.Session()\n",
        "    role = get_execution_role()\n",
        "    \n",
        "    # Set S3 bucket\n",
        "    bucket = args.s3_bucket if args.s3_bucket else session.default_bucket()\n",
        "    prefix = args.s3_prefix\n",
        "    \n",
        "    print(f\"Using S3 bucket: {bucket}\")\n",
        "    \n",
        "    # Format data for Llama 3.2\n",
        "    formatted_file = \"formatted_data.jsonl\"\n",
        "    format_data_for_llama(args.input_file, formatted_file)\n",
        "    \n",
        "    # Upload data to S3\n",
        "    train_data_s3_path = session.upload_data(\n",
        "        path=formatted_file,\n",
        "        bucket=bucket,\n",
        "        key_prefix=f\"{prefix}/data\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Training data uploaded to: {train_data_s3_path}\")\n",
        "    \n",
        "    # Define model ID for Llama 3.2 3B\n",
        "    model_id = \"meta-textgeneration-llama-3-2-3b-instruct\"\n",
        "    model_version = \"*\"\n",
        "    \n",
        "    # Configure hyperparameters\n",
        "    hyperparameters = {\n",
        "        \"epoch\": str(args.epochs),\n",
        "        \"learning_rate\": str(args.learning_rate),\n",
        "        \"per_device_train_batch_size\": str(args.batch_size),\n",
        "        \"gradient_accumulation_steps\": \"2\",\n",
        "        \"max_seq_length\": \"2048\",\n",
        "        \n",
        "        # LoRA parameters\n",
        "        \"lora_alpha\": \"32\",\n",
        "        \"lora_dropout\": \"0.05\",\n",
        "        \"lora_r\": \"16\",\n",
        "        \n",
        "        # QLoRA and PEFT settings\n",
        "        \"use_bnb_4bit\": \"True\",\n",
        "        \"use_peft\": \"True\",\n",
        "        \n",
        "        # Training settings\n",
        "        \"save_strategy\": \"epoch\",\n",
        "        \"warmup_ratio\": \"0.03\",\n",
        "        \"push_to_hub\": \"False\",\n",
        "    }\n",
        "    \n",
        "    # Create JumpStart estimator\n",
        "    estimator = JumpStartEstimator(\n",
        "        model_id=model_id,\n",
        "        model_version=model_version,\n",
        "        role=role,\n",
        "        instance_count=1,\n",
        "        instance_type=args.instance_type,\n",
        "        hyperparameters=hyperparameters,\n",
        "        environment={\"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"3600\"},\n",
        "        disable_output_compression=True\n",
        "    )\n",
        "    \n",
        "    # Start training\n",
        "    print(\"Starting fine-tuning job...\")\n",
        "    estimator.fit(\n",
        "        train_data_s3_path,\n",
        "        logs=True,\n",
        "        wait=True\n",
        "    )\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "    print(f\"Model artifacts saved to: {estimator.model_data}\")\n",
        "    \n",
        "    # Deploy model if requested\n",
        "    if args.deploy:\n",
        "        print(f\"Deploying model to endpoint using instance type: {args.deploy_instance_type}\")\n",
        "        predictor = estimator.deploy(\n",
        "            initial_instance_count=1,\n",
        "            instance_type=args.deploy_instance_type,\n",
        "            container_startup_health_check_timeout=600\n",
        "        )\n",
        "        \n",
        "        endpoint_name = predictor.endpoint_name\n",
        "        print(f\"Model deployed successfully to endpoint: {endpoint_name}\")\n",
        "        \n",
        "        # Save endpoint name to file for future reference\n",
        "        with open(\"endpoint_info.json\", \"w\") as f:\n",
        "            json.dump({\"endpoint_name\": endpoint_name}, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Script Usage Example\n",
        "\n",
        "You can run the script with the following command:\n",
        "\n",
        "```bash\n",
        "python finetune_llama_jumpstart.py \\\n",
        "    --input-file your_data.jsonl \\\n",
        "    --s3-bucket your-bucket-name \\\n",
        "    --epochs 3 \\\n",
        "    --learning-rate 2e-4 \\\n",
        "    --batch-size 4 \\\n",
        "    --deploy\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up Resources\n",
        "\n",
        "Remember to clean up resources when you're done to avoid unnecessary charges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Delete endpoint if it was created\n",
        "try:\n",
        "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
        "    print(\"Endpoint deleted successfully.\")\n",
        "except NameError:\n",
        "    print(\"No endpoint to delete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
