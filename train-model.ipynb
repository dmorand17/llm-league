{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3.2 3B Instruct with SageMaker JumpStart\n",
    "\n",
    "This notebook demonstrates how to fine-tune Meta's Llama 3.2 3B Instruct model using Amazon SageMaker JumpStart. We'll use a small dataset (around 100 examples) and parameter-efficient fine-tuning techniques like LoRA/QLoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's set up our SageMaker environment and install any required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"AWS Region: {region}\")\n",
    "print(f\"Default S3 Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Let's create a simple example dataset in JSONL format. For real use cases, you would replace this with your actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset or read from existing file\n",
    "# Example of a chat-based jsonl format compatible with Llama models\n",
    "sample_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What are the benefits of fine-tuning language models?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Fine-tuning language models offers several benefits, including improved performance on domain-specific tasks, better alignment with specific use cases, reduced hallucinations, and more controlled outputs that follow your preferred style and format.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How does parameter-efficient fine-tuning work?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Parameter-efficient fine-tuning methods like LoRA (Low-Rank Adaptation) work by freezing most of the pre-trained model parameters and only updating a small set of adapter parameters. LoRA adds trainable low-rank matrices to certain layers, typically the attention layers, which significantly reduces memory requirements while maintaining performance.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a local file with the sample data\n",
    "sample_data_file = \"sample_training_data.jsonl\"\n",
    "with open(sample_data_file, 'w') as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "print(f\"Created sample training data with {len(sample_data)} examples\")\n",
    "\n",
    "# For real data, you might load from a file like this:\n",
    "# import pandas as pd\n",
    "# df = pd.read_json(\"your_real_data.jsonl\", lines=True)\n",
    "# print(f\"Loaded {len(df)} training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data to S3\n",
    "prefix = \"llama3-finetuning\"\n",
    "train_data_s3_path = session.upload_data(\n",
    "    path=sample_data_file, \n",
    "    bucket=bucket, \n",
    "    key_prefix=f\"{prefix}/data\"\n",
    ")\n",
    "print(f\"Training data uploaded to: {train_data_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hyperparameters\n",
    "\n",
    "Let's set up the hyperparameters for fine-tuning. These are optimized for a small dataset (around 100 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    # Training parameters\n",
    "    \"epoch\": \"3\",                  # Number of training epochs\n",
    "    \"learning_rate\": \"5e-5\",       # Learning rate\n",
    "    \"per_device_train_batch_size\": \"2\",  # Batch size per GPU for training\n",
    "    \"per_device_eval_batch_size\": \"2\",   # Batch size per GPU for evaluation\n",
    "    \"gradient_accumulation_steps\": \"4\",  # Number of steps to accumulate gradients\n",
    "    \"warmup_steps\": \"10\",          # Number of warmup steps for learning rate scheduler\n",
    "    \"weight_decay\": \"0.01\",        # Weight decay\n",
    "    \n",
    "    # LoRA specific parameters\n",
    "    \"use_lora\": \"True\",            # Use LoRA for fine-tuning\n",
    "    \"lora_r\": \"16\",                # LoRA attention dimension\n",
    "    \"lora_alpha\": \"32\",            # LoRA alpha parameter\n",
    "    \"lora_dropout\": \"0.05\",        # Dropout probability for LoRA layers\n",
    "    \n",
    "    # QLoRA specific parameters (for memory efficiency)\n",
    "    \"use_qlora\": \"True\",           # Use QLoRA for more memory efficiency\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",  # Quantization type\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",  # Compute dtype\n",
    "    \n",
    "    # Other settings\n",
    "    \"max_seq_length\": \"2048\",      # Maximum sequence length\n",
    "    \"save_strategy\": \"epoch\",      # Save strategy\n",
    "    \"evaluation_strategy\": \"epoch\" # Evaluation strategy\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters configured for small dataset fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Start the Training Job\n",
    "\n",
    "Now we'll use SageMaker JumpStart to create and start the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model ID for Llama 3.2 3B Instruct\n",
    "model_id = \"meta-textgeneration-llama-3-2-3b-instruct\"\n",
    "model_version = \"1.0.0\"  # Update this version as needed\n",
    "\n",
    "# Create JumpStart estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.2xlarge\",  # GPU instance with good memory\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "print(f\"Created JumpStart estimator for {model_id} version {model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training job\n",
    "training_input = {\"train\": train_data_s3_path}\n",
    "\n",
    "# Generate a unique job name\n",
    "import time\n",
    "job_name = f\"llama3-2-3b-finetuning-{int(time.time())}\"\n",
    "\n",
    "# Start the training job\n",
    "estimator.fit(\n",
    "    inputs=training_input,\n",
    "    job_name=job_name,\n",
    "    wait=False,  # Set to True if you want to wait for job completion in the notebook\n",
    "    logs=False   # Set to True if you want to see logs in the notebook\n",
    ")\n",
    "\n",
    "print(f\"Training job '{job_name}' started!\")\n",
    "print(f\"You can monitor the job in the SageMaker console or run 'estimator.latest_training_job.wait()' to wait for completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "training_job_name = estimator.latest_training_job.job_name\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "\n",
    "# Get training job status\n",
    "sm_client = boto3.client('sagemaker')\n",
    "response = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "print(f\"Training job status: {response['TrainingJobStatus']}\")\n",
    "\n",
    "# If you want to wait for the job to complete\n",
    "# estimator.latest_training_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Fine-tuned Model (After Training Completes)\n",
    "\n",
    "Once the training job completes, you can deploy the model as an endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this after training completes\n",
    "\n",
    "# # Deploy the fine-tuned model\n",
    "# predictor = estimator.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=\"ml.g5.xlarge\",\n",
    "#     endpoint_name=f\"llama3-2-finetuned-endpoint-{int(time.time())}\"\n",
    "# )\n",
    "# \n",
    "# print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model (After Deployment)\n",
    "\n",
    "After deployment, you can test your fine-tuned model with inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this after deployment completes\n",
    "\n",
    "# # Test the deployed model\n",
    "# prompt = [{\"role\": \"user\", \"content\": \"What are the key benefits of fine-tuning language models?\"}]\n",
    "# \n",
    "# response = predictor.predict({\n",
    "#     \"inputs\": prompt,\n",
    "#     \"parameters\": {\n",
    "#         \"max_new_tokens\": 256,\n",
    "#         \"top_p\": 0.9,\n",
    "#         \"temperature\": 0.7\n",
    "#     }\n",
    "# })\n",
    "# \n",
    "# print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "\n",
    "Don't forget to clean up resources when you're done to avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this when you're done\n",
    "\n",
    "# # Delete the endpoint\n",
    "# predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "# print(f\"Deleted endpoint: {predictor.endpoint_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
